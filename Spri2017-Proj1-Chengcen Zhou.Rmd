---
title: 'Prediting Donald Trump Inauguration Lyrics Using Big Data '
runtime: shiny
output:
  html_document: default
  html_notebook: default
---
\\
Here I conduct a brief study of what did the president say at their inauguation. The data collects 59 Inaugurals. I focus on using past presidents' speeches to predict Donald Trump's Inaugural.
\\

#load libraries
```{r, message=FALSE, warning=FALSE}
library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)
library(SnowballC)
```


#Inaugural Wordcloud between two Parties (demo and repub)
```{r, message=FALSE, warning=FALSE}
# All
folder.path="/Users/Connie/Desktop/Inaugural"
speeches=list.files(path = folder.path, pattern = "*.txt")
prex.out=substr(speeches, 59, nchar(speeches)-4)

ff.all=Corpus(DirSource(folder.path))

#Sys.setlocale("LC_ALL", "C")
ff.all=tm_map(ff.all, removeNumbers,lazy=TRUE)  
ff.all=tm_map(ff.all, stripWhitespace,lazy=TRUE)
ff.all=tm_map(ff.all, content_transformer(tolower),lazy=TRUE)
ff.all=tm_map(ff.all, removeWords, stopwords("english"),lazy=TRUE)
ff.all=tm_map(ff.all, removeWords, character(0),lazy=TRUE)
ff.all=tm_map(ff.all, removePunctuation,lazy=TRUE)


tdm.all=DocumentTermMatrix(ff.all,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, 
                                                         normalize =FALSE),
                                         stopwords = TRUE))


ff.dtm=tidy(tdm.all)
#save(tdm.tidy,file="tdm_all.RData")

# Demo #      Same method but divide into two folders  #####################
folder.path="/Users/Connie/Downloads/Wordcloud/Demo"
ff.demo=Corpus(DirSource(folder.path))

Sys.setlocale("LC_ALL", "C")
ff.demo=tm_map(ff.demo, removeNumbers,lazy=TRUE)  
ff.demo=tm_map(ff.demo, stripWhitespace,lazy=TRUE)
ff.demo=tm_map(ff.demo, content_transformer(tolower),lazy=TRUE)
ff.demo=tm_map(ff.demo, removeWords, stopwords("english"),lazy=TRUE)
ff.demo=tm_map(ff.demo, removeWords, character(0),lazy=TRUE)
ff.demo=tm_map(ff.demo, removePunctuation,lazy=TRUE)

#TF-IDF weighted
tdm.demo=DocumentTermMatrix(ff.demo,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, 
                                                        normalize =FALSE),
                                         stopwords = TRUE))

tdm.tidy.demo=tidy(tdm.demo)

tdm.demo1=summarise(group_by(tdm.tidy.demo, term), sum(count))

#save(tdm.demo1,file="tdm.demo1.RData")

#load("tdm.demo1.RData")

wordcloud(tdm.demo1$term, tdm.demo1$`sum(count)`,
          scale=c(4,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))
# Repub #####################################################
folder.path="/Users/Connie/Downloads/Wordcloud/Repub"
ff.repub=Corpus(DirSource(folder.path))

Sys.setlocale("LC_ALL", "C")
ff.repub=tm_map(ff.repub, removeNumbers,lazy=TRUE)  
ff.repub=tm_map(ff.repub, stripWhitespace,lazy=TRUE)
ff.repub=tm_map(ff.repub, content_transformer(tolower),lazy=TRUE)
ff.repub=tm_map(ff.repub, removeWords, stopwords("english"),lazy=TRUE)
ff.repub=tm_map(ff.repub, removeWords, character(0),lazy=TRUE)
ff.repub=tm_map(ff.repub, removePunctuation,lazy=TRUE)

tdm.repub=DocumentTermMatrix(ff.repub,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, 
                                                         normalize =FALSE),
                                         stopwords = TRUE))

tdm.tidy.repub=tidy(tdm.repub)

tdm.repub1=summarise(group_by(tdm.tidy.repub, term), sum(count))
#save(tdm.repub1,file="tdm.repub1.RData")
#load("tdm.repub1.RData")
wordcloud(tdm.repub1$term, tdm.repub1$`sum(count)`,
          scale=c(4,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,'YlOrRd'))
```


#Shiny wordcloud
\
As we know that the Democrat is based on community and social responsibility. the Republican based on individual rights and justice. Obama focused on "journey", "Creed", "Self-evident", "complete". I found it interesting to study all president's high frequency word in the speech. \


```{r, warning=FALSE}
library(shiny)

shinyApp(
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",
        column(4, selectInput('speech1', 'Speech 1',
                              speeches,
                              selected=speeches[5])),
        column(4, selectInput('speech2', 'Speech 2', speeches,
                              selected=speeches[9])),
        column(4, sliderInput('nwords', 'Number of words', 3,
                               min = 20, max = 200, value=100, step = 20))
      ),
      fluidRow(
        plotOutput('wordclouds', height = "400px")
      )
    ),

    server = function(input, output, session) {

      # Combine the selected variables into a new data frame
      selectedData <- reactive({
        list(dtm.term1=ff.dtm$term[ff.dtm$document==as.character(input$speech1)],
             dtm.count1=ff.dtm$count[ff.dtm$document==as.character(input$speech1)],
             dtm.term2=ff.dtm$term[ff.dtm$document==as.character(input$speech2)],
             dtm.count2=ff.dtm$count[ff.dtm$document==as.character(input$speech2)])
      })

      output$wordclouds <- renderPlot(height = 400, {
        par(mfrow=c(1,2), mar = c(0, 0, 3, 0))
        wordcloud(selectedData()$dtm.term1, 
                  selectedData()$dtm.count1,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(9,"Blues"), 
            main=input$speech1)
        wordcloud(selectedData()$dtm.term2, 
                  selectedData()$dtm.count2,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(9,"Blues"), 
            main=input$speech2)
      })
    },

    options = list(height = 600)
)
```
#Shiny-Sentiment

\
We could mainly divide the sentiment into several parts including anger anticipation, disgust, fear, joy, negative, positive, surprise, trust.\

```{r,  message=FALSE, warning=FALSE}
library(shiny)
library(syuzhet)
library(ggplot2)
library(plotly)
#devtools::install_github("ropensci/plotly") 

shinyApp(
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",
        column(4, selectInput('speech1', 'Speech 1',
                              speeches,
                              selected=speeches[5])),
        column(4, selectInput('speech2', 'Speech 2', speeches,
                              selected=speeches[9]))
      ),
      fluidRow(
        plotOutput('sentiments', height = "500px")
      )
    ),

    server = function(input, output, session) {

      # Combine the selected variables into a new data frame
      selectedData <- reactive({
        list(dtm.term1=ff.dtm$term[ff.dtm$document==as.character(input$speech1)],
             dtm.term2=ff.dtm$term[ff.dtm$document==as.character(input$speech2)])
      })

      output$sentiments <- renderPlot(height = 400, {
        par(mfrow=c(1,2), mar = c(3, 0, 3, 0))
        
        dtm.term1_Senti=get_nrc_sentiment(selectedData()$dtm.term1)
        dtm.term1_Senti_tot=data.frame(colSums(dtm.term1_Senti))
        names(dtm.term1_Senti_tot)="count"
        senti_tot=cbind("sentiment"=rownames(dtm.term1_Senti_tot), dtm.term1_Senti_tot)
        rownames(dtm.term1_Senti_tot)=NULL
        ggplot(data=dtm.term1_Senti_tot, aes(x = sentiment, y = count)) +
            geom_bar(aes(fill = sentiment), stat = "identity") +
            theme(legend.position = "none") +
            xlab("Sentiment") + ylab("Total Count") + ggtitle("Total Sentiment Score")
        
        
        dtm.term2_Senti=get_nrc_sentiment(selectedData()$dtm.term2)
        dtm.term2_Senti_tot=data.frame(colSums(dtm.term2_Senti))
        names(dtm.term2_Senti_tot)="count"
        dtm.term2_Senti_tot=cbind("sentiment"=rownames(dtm.term2_Senti_tot), dtm.term2_Senti_tot)
        rownames(dtm.term2_Senti_tot)=NULL
        ggplot(data=dtm.term2_Senti_tot, aes(x = sentiment, y = count)) +
            geom_bar(aes(fill = sentiment), stat = "identity") +
            theme(legend.position = "none") +
            xlab("Sentiment") + ylab("Total Count") + ggtitle("Total Sentiment Score")
      })
    },

    options = list(height = 600)
)
```

\
Most sentiments are positive. As you can see the blue color indicates positive, it usually appears at the last parts of the speech. When we predict Trump's speech for this year, we could find the core idea of Republicain. And then find the pattern of how presidents give speech. One interesting thing is, an outstanding elocutionist unintentionally prefer to use short sentences instead of long statements.  That's to say, Trump might use a lot short sentences. It makes people feel they are talking to the speaker not listening to a dull speech. \

\
The thing is when comparing with Trump's inaugural, he did mentioned what big data told us, however, the topics are just regular part. From the sentiment test, we can see that Trump’s words indicate more anger and fear than other presidents did in the past. For instance, he started the speech with fear “For too long, a small group in our nation's capital has reaped the rewards of government while the people have borne the cost. Washington flourished, but the people did not share in its wealth. ” Big data told us regular parts of the speech. It can’t exactly predict the whole inaugural. Artificial intelligence can’t represent real human beings. At least, we made some progress on it. There are still many ways that we could continue improving big data methods in the future.\ 
